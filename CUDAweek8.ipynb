{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UW14Y40RGfrzkjlu7GPQcRsk4Ts7Nqke",
      "authorship_tag": "ABX9TyNtI/VcPjr2dKhoMSzf97OI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShashwotP/GroupE_G12/blob/main/CUDAweek8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAmw3cf4EpKp",
        "outputId": "94c3e54c-06e0-4dfc-83d9-81627f3836b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 24 09:07:23 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAB71ViBy6Oh",
        "outputId": "29ec3604-cea5-489f-a401-eeceb6fd0795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y0AEqLeFSrR",
        "outputId": "77c72740-4524-4390-a4fa-266c9312bb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile firstC.cu\n",
        "#include <stdio.h>\n",
        "int main(void)\n",
        "{\n",
        "  printf(\"Hello world from CPU!\\n\");\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjCQj9qvFcd1",
        "outputId": "b47e75ac-4118-4c3e-f7d2-dcdc1d600a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing firstC.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc firstC.cu -o 1"
      ],
      "metadata": {
        "id": "fLq4it0JFzph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG0UBuYyF99U",
        "outputId": "bcdc9999-9dda-4a25-b4f0-8041d38bcb11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world from CPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello.cu\n",
        "#include <stdio.h>\n",
        "__global__ void hello(void)\n",
        "{\n",
        "  printf(\"Hello world from CPU!\\n\");\n",
        "\n",
        "}\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  printf(\"CPU: Hello!:\\n\");\n",
        "  hello<<<1,10>>>();\n",
        "  cudaDeviceReset();\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_zzovMAGBCS",
        "outputId": "422eed61-d3f8-4fd5-cd4c-4e3cbbd2f549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hello.cu -o hello"
      ],
      "metadata": {
        "id": "gCFlrnzOGtV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxw40iQnHghg",
        "outputId": "6168cde7-1d2d-42c3-e87b-fbaedebc134f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: Hello!:\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n",
            "Hello world from CPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dq00.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "/*****************************************************************************\n",
        " * This program checks if it is being run on a computer with a CUDA compatible\n",
        " * GPU, i.e. a modern nVidia GPU. If this program reports there are no\n",
        " * devices then no other programs in this set are going to work on the machine\n",
        " * being run on.\n",
        " *\n",
        " * Compile with:\n",
        " *   nvcc -o dq00 dq00.cu\n",
        " *\n",
        " * Dr Kevan Buckley, University of Wolverhampton, 2018\n",
        " ****************************************************************************/\n",
        "int main() {\n",
        "  int device_count;\n",
        "  cudaError_t error_id = cudaGetDeviceCount(&device_count);\n",
        "\n",
        "  if (error_id != 0) {\n",
        "    fprintf(stderr, \"cudaGetDeviceCount returned %d\\n-> %s\\n\",\n",
        "           (int)error_id, cudaGetErrorString(error_id));\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  if (device_count == 0) {\n",
        "    fprintf(stderr, \"There are no available device(s) that support CUDA\\n\");\n",
        "  } else {\n",
        "    printf(\"Detected %d CUDA capable device(s)\\n\", device_count);\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcX1KHDoK8qc",
        "outputId": "17b6d878-4c95-4cb5-983e-69e9c34109a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dq00.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o dq00 dq00.cu"
      ],
      "metadata": {
        "id": "-9DvpbNDMXda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./dq00"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss6FP4zOMgs3",
        "outputId": "43493feb-3bc7-47be-91b0-560a24477b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 1 CUDA capable device(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dq01.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "/*****************************************************************************\n",
        " * This program introduces using cudaGetDeviceProperties to find about the\n",
        " * available GPUs. Information is returned in a cudaDeviceProp data structure\n",
        " * which can be found, with a limited amount of documentation in\n",
        " * cuda_runtim_api.h. A lot of the fields are self explanatory so the struct\n",
        " * declaration is copied below for convenience. The next few programs in this\n",
        " * set demonstrate using this information. When you have tried them you could\n",
        " * look at investigating some of the other available fields and by reading\n",
        " * about what they mean gain a greater understanding of CUDA and GPU\n",
        " * architecture.\n",
        " *\n",
        "    struct cudaDeviceProp {\n",
        "      char name[256];\n",
        "      size_t totalGlobalMem;\n",
        "      size_t sharedMemPerBlock;\n",
        "      int regsPerBlock;\n",
        "      int warpSize;\n",
        "      size_t memPitch;\n",
        "      int maxThreadsPerBlock;\n",
        "      int maxThreadsDim[3];\n",
        "      int maxGridSize[3];\n",
        "      int clockRate;\n",
        "      size_t totalConstMem;\n",
        "      int major;\n",
        "      int minor;\n",
        "      size_t textureAlignment;\n",
        "      size_t texturePitchAlignment;\n",
        "      int deviceOverlap;\n",
        "      int multiProcessorCount;\n",
        "      int kernelExecTimeoutEnabled;\n",
        "      int integrated;\n",
        "      int canMapHostMemory;\n",
        "      int computeMode;\n",
        "      int maxTexture1D;\n",
        "      int maxTexture1DMipmap;\n",
        "      int maxTexture1DLinear;\n",
        "      int maxTexture2D[2];\n",
        "      int maxTexture2DMipmap[2];\n",
        "      int maxTexture2DLinear[3];\n",
        "      int maxTexture2DGather[2];\n",
        "      int maxTexture3D[3];\n",
        "      int maxTexture3DAlt[3];\n",
        "      int maxTextureCubemap;\n",
        "      int maxTexture1DLayered[2];\n",
        "      int maxTexture2DLayered[3];\n",
        "      int maxTextureCubemapLayered[2];\n",
        "      int maxSurface1D;\n",
        "      int maxSurface2D[2];\n",
        "      int maxSurface3D[3];\n",
        "      int maxSurface1DLayered[2];\n",
        "      int maxSurface2DLayered[3];\n",
        "      int maxSurfaceCubemap;\n",
        "      int maxSurfaceCubemapLayered[2];\n",
        "      size_t surfaceAlignment;\n",
        "      int concurrentKernels;\n",
        "      int ECCEnabled;\n",
        "      int pciBusID;\n",
        "      int pciDeviceID;\n",
        "      int pciDomainID;\n",
        "      int tccDriver;\n",
        "      int asyncEngineCount;\n",
        "      int unifiedAddressing;\n",
        "      int memoryClockRate;\n",
        "      int memoryBusWidth;\n",
        "      int l2CacheSize;\n",
        "      int maxThreadsPerMultiProcessor;\n",
        "      int streamPrioritiesSupported;\n",
        "      int globalL1CacheSupported;\n",
        "      int localL1CacheSupported;\n",
        "      size_t sharedMemPerMultiprocessor;\n",
        "      int regsPerMultiprocessor;\n",
        "      int managedMemSupported;\n",
        "      int isMultiGpuBoard;\n",
        "      int multiGpuBoardGroupID;\n",
        "      int singleToDoublePrecisionPerfRatio;\n",
        "      int pageableMemoryAccess;\n",
        "      int concurrentManagedAccess;\n",
        "    }\n",
        " *\n",
        " * Compile with:\n",
        " *   nvcc -o dq01 dq01.cu\n",
        " *\n",
        " * Dr Kevan Buckley, University of Wolverhampton, 2018\n",
        " ****************************************************************************/\n",
        "\n",
        "\n",
        "int main() {\n",
        "\n",
        "  // cudaSetDevice(0); // only needed if there are multiple GPUs\n",
        "\n",
        "  cudaDeviceProp device_properties;\n",
        "  cudaGetDeviceProperties(&device_properties, 0);\n",
        "\n",
        "  int driver_version, runtime_version;\n",
        "  cudaDriverGetVersion(&driver_version);\n",
        "  cudaRuntimeGetVersion(&runtime_version);\n",
        "\n",
        "  printf(\"Device 0 is a \\\"%s\\\"\\n\", device_properties.name);\n",
        "  printf(\"  Compute capability %d.%d\\n\", device_properties.major,\n",
        "                                         device_properties.minor);\n",
        "\n",
        "  printf(\"  CUDA Driver version %d.%d\\n\",\n",
        "         driver_version/1000, (driver_version%100)/10);\n",
        "\n",
        "  printf(\"  CUDA runtime version %d.%d\\n\",\n",
        "         runtime_version/1000, (runtime_version%100)/10);\n",
        "\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQE3U1wJNDTe",
        "outputId": "6cc86d8a-bdc9-4183-e548-fae564c18c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dq01.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o dq01 dq01.cu"
      ],
      "metadata": {
        "id": "wiJlWLb0OR1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./dq01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqjtdj1bOY7h",
        "outputId": "f17a01e2-17ca-4a8f-a40a-11cbe9094699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device 0 is a \"Tesla T4\"\n",
            "  Compute capability 7.5\n",
            "  CUDA Driver version 12.2\n",
            "  CUDA runtime version 12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dq02.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "/*****************************************************************************\n",
        " * The most complicated thing this program does is work out how many CUDA\n",
        " * cores are available. This is based on the particular architecture found.\n",
        " * Documentation for this is difficult to find. The\n",
        " * convert_compute_capability_to_cores function used here was copied from the\n",
        " * examples that come with CUDA (but other versions exist on the Internet with\n",
        " * different copyright notices!). This code really over complicates things so\n",
        " * just take it for granted that it works. It will need to be updated as new\n",
        " * GPU architectures become available and is known to be out of date at the\n",
        " * time this source code set was put together, i.e. the Volta architecture\n",
        " * has superceded Pascal (found in GTX1000 series consumer GPUs, such as the\n",
        " * GTX1070s in wlv labs)\n",
        " *\n",
        " * Compile with:\n",
        " *   nvcc -o dq02 dq02.cu\n",
        " *\n",
        " * Dr Kevan Buckley, University of Wolverhampton, 2018\n",
        " ****************************************************************************/\n",
        "\n",
        "// This function which is heavily  based on\n",
        "// NVIDIA_CUDA-8.0_Samples/common/inc/helper_cuda.h _ConvertSMVer2Cores()\n",
        "// takes a compute capability (in the form of 2 parameters and returns the\n",
        "// number of cores per multiprocessor. The algorithm is complicated by\n",
        "// not all major, minor pairs being valid it might be more readable using a\n",
        "// sparse array, but to aid the update of this code when a new architecture\n",
        "// comes out as little as possible has been changed.\n",
        "\n",
        "int convert_compute_capability_to_cores(int major, int minor) {\n",
        "    // Defines for GPU Architecture types (using the SM version to determine\n",
        "    // the # of cores per SM\n",
        "    typedef struct {\n",
        "        int SM; // 0xMm (hexidecimal notation)\n",
        "                // M = SM Major version\n",
        "                // m = SM minor version\n",
        "        int Cores;\n",
        "    } sSMtoCores;\n",
        "\n",
        "    sSMtoCores nGpuArchCoresPerSM[] = {\n",
        "      { 0x20, 32 }, // Fermi Generation (SM 2.0) GF100 class\n",
        "      { 0x21, 48 }, // Fermi Generation (SM 2.1) GF10x class\n",
        "      { 0x30, 192}, // Kepler Generation (SM 3.0) GK10x class\n",
        "      { 0x32, 192}, // Kepler Generation (SM 3.2) GK10x class\n",
        "      { 0x35, 192}, // Kepler Generation (SM 3.5) GK11x class\n",
        "      { 0x37, 192}, // Kepler Generation (SM 3.7) GK21x class\n",
        "      { 0x50, 128}, // Maxwell Generation (SM 5.0) GM10x class\n",
        "      { 0x52, 128}, // Maxwell Generation (SM 5.2) GM20x class\n",
        "      { 0x53, 128}, // Maxwell Generation (SM 5.3) GM20x class\n",
        "      { 0x60, 64 }, // Pascal Generation (SM 6.0) GP100 class\n",
        "      { 0x61, 128}, // Pascal Generation (SM 6.1) GP10x class\n",
        "      { 0x62, 128}, // Pascal Generation (SM 6.2) GP10x class\n",
        "      {   -1, -1 }\n",
        "    };\n",
        "\n",
        "    int index = 0;\n",
        "\n",
        "    while (nGpuArchCoresPerSM[index].SM != -1) {\n",
        "      if (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor)) {\n",
        "          return nGpuArchCoresPerSM[index].Cores;\n",
        "      }\n",
        "\n",
        "      index++;\n",
        "    }\n",
        "\n",
        "    // If we don't find the values, we default use the previous one to run properly\n",
        "    printf(\"MapSMtoCores for SM %d.%d is undefined. Default to use %d Cores/SM\\n\",\n",
        "           major, minor, nGpuArchCoresPerSM[index-1].Cores);\n",
        "    return nGpuArchCoresPerSM[index-1].Cores;\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "\n",
        "  cudaDeviceProp device_properties;\n",
        "  cudaGetDeviceProperties(&device_properties, 0);\n",
        "\n",
        "  printf(\"Device 0 is a \\\"%s\\\"\\n\", device_properties.name);\n",
        "\n",
        "  printf(\"  Clock rate = %d MHz\\n\", device_properties.clockRate/1000);\n",
        "  printf(\"  Memory = %lu bytes\\n\", device_properties.totalGlobalMem);\n",
        "\n",
        "  int cores_per_multiprocessor =\n",
        "    convert_compute_capability_to_cores(device_properties.major,\n",
        "                                        device_properties.minor);\n",
        "  printf(\"  Number of multi processors = %d\\n\",\n",
        "         device_properties.multiProcessorCount);\n",
        "\n",
        "  printf(\"  Cores per multiprocessors = %d\\n\",\n",
        "         cores_per_multiprocessor);\n",
        "\n",
        "  printf(\"  Total CUDA cores = %d\\n\",\n",
        "         cores_per_multiprocessor * device_properties.multiProcessorCount);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqF6-imcOfWb",
        "outputId": "e96b4c12-7548-4545-ea20-4219ce774e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dq02.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o dq02 dq02.cu"
      ],
      "metadata": {
        "id": "0_c8zAtsPHjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./dq02"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAkKpTc_PUym",
        "outputId": "23fdd591-1a20-4e1d-d4f3-c62b9255ae66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device 0 is a \"Tesla T4\"\n",
            "  Clock rate = 1590 MHz\n",
            "  Memory = 15835660288 bytes\n",
            "MapSMtoCores for SM 7.5 is undefined. Default to use 128 Cores/SM\n",
            "  Number of multi processors = 40\n",
            "  Cores per multiprocessors = 128\n",
            "  Total CUDA cores = 5120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dq03.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "/*****************************************************************************\n",
        " * This program introduces using cudaGetDeviceProperties to find about the\n",
        " * available GPUs. This version of the program concentrates on the\n",
        " * multithreading capabilities: *\n",
        "\n",
        "      int maxThreadsPerBlock;\n",
        "      int maxThreadsDim[3];\n",
        "      int maxGridSize[3];\n",
        "\n",
        " *\n",
        " * Compile with:\n",
        " *   nvcc -o dq03 dq03.cu\n",
        " *\n",
        " * Dr Kevan Buckley, University of Wolverhampton, 2018\n",
        " ****************************************************************************/\n",
        "\n",
        "\n",
        "int main() {\n",
        "\n",
        "  cudaDeviceProp device_properties;\n",
        "  cudaGetDeviceProperties(&device_properties, 0);\n",
        "\n",
        "  printf(\"Maximum number of threads per block = %d\\n\",\n",
        "          device_properties.maxThreadsPerBlock);\n",
        "\n",
        "  printf(\"Maximum size of each dimension of a block = [%d][%d][%d]\\n\",\n",
        "          device_properties.maxThreadsDim[0],\n",
        "          device_properties.maxThreadsDim[1],\n",
        "          device_properties.maxThreadsDim[2]);\n",
        "\n",
        "  printf(\"Maximum size of each dimension of a grid = [%d][%d][%d]\\n\",\n",
        "          device_properties.maxGridSize[0],\n",
        "          device_properties.maxGridSize[1],\n",
        "          device_properties.maxGridSize[2]);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdU7tkG_QEvh",
        "outputId": "a6f7c2ea-3ac6-4758-dc86-9b84db83761e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dq03.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o dq03 dq03.cu"
      ],
      "metadata": {
        "id": "64vieeTyQYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./dq03"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBcrCN10Qq7U",
        "outputId": "d7449d50-9b52-4680-909a-69719473ffa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of threads per block = 1024\n",
            "Maximum size of each dimension of a block = [1024][1024][64]\n",
            "Maximum size of each dimension of a grid = [2147483647][65535][65535]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile factorise_3_cude.cu\n",
        "/****************************************************************************\n",
        "  Similar to the factorise programs studied with POSIX threads, only this\n",
        "  version runs on a GPU with CUDA.\n",
        "\n",
        "  Compile with:\n",
        "    nvcc -o factorise_3_cuda factorise_3_cuda.cu -lrt\n",
        "\n",
        "  Dr Kevan Buckley, University of Wolverhampton, 2018\n",
        "*****************************************************************************/\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <ctype.h>\n",
        "#include <errno.h>\n",
        "#include <sys/stat.h>\n",
        "#include <string.h>\n",
        "#include <time.h>\n",
        "//#include <pthread.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define goal 98931313\n",
        "\n",
        "__global__ void factorise(){\n",
        "  int a = threadIdx.x;\n",
        "  int b = blockIdx.x;\n",
        "  int c = blockIdx.y;\n",
        "\n",
        "  if(a*b*c == goal){\n",
        "     printf(\"solution is %d, %d, %d\\n\", a, b, c);\n",
        "  }\n",
        "}\n",
        "\n",
        "int time_difference(struct timespec *start, struct timespec *finish,\n",
        "                              long long int *difference) {\n",
        "  long long int ds =  finish->tv_sec - start->tv_sec;\n",
        "  long long int dn =  finish->tv_nsec - start->tv_nsec;\n",
        "\n",
        "  if(dn < 0 ) {\n",
        "    ds--;\n",
        "    dn += 1000000000;\n",
        "  }\n",
        "  *difference = ds * 1000000000 + dn;\n",
        "  return !(*difference > 0);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  cudaError_t error;\n",
        "  struct timespec start, finish;\n",
        "  long long int time_elapsed;\n",
        "\n",
        "  //clock_gettime(CLOCK_MONOTONIC, &start);\n",
        "\n",
        "  dim3 gd(1000, 1000, 1);\n",
        "  dim3 bd(1000, 1, 1);\n",
        "  factorise<<<gd, bd>>>();\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  error = cudaGetLastError();\n",
        "\n",
        "  if(error){\n",
        "    fprintf(stderr, \"Kernel launch returned %d %s\\n\",\n",
        "      error, cudaGetErrorString(error));\n",
        "    return 1;\n",
        "  } else {\n",
        "    fprintf(stderr, \"Kernel launch successful.\\n\");\n",
        "  }\n",
        "  //clock_gettime(CLOCK_MONOTONIC, &finish);\n",
        "  time_difference(&start, &finish, &time_elapsed);\n",
        "  printf(\"Time elapsed was %lldns or %0.9lfs\\n\",\n",
        "    time_elapsed, (time_elapsed/1.0e9));\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CflZWJM5QvkS",
        "outputId": "1e15c925-dc8f-4672-a4cd-8050a074f6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing factorise_3_cude.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o factorise_3_cude  factorise_3_cude.cu"
      ],
      "metadata": {
        "id": "QFPx_TazRC1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./factorise_3_cude"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y_7zu8uRGfh",
        "outputId": "8ae8d9e0-d67e-462d-95ea-ece8fcaefa2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solution is 997, 449, 221\n",
            "solution is 449, 997, 221\n",
            "solution is 997, 221, 449\n",
            "solution is 221, 997, 449\n",
            "solution is 449, 221, 997\n",
            "solution is 221, 449, 997\n",
            "Kernel launch successful.\n",
            "Time elapsed was 0ns or 0.000000000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 01.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "/******************************************************************************\n",
        "  Very simple CUDA program that shows the principles of copying data to and\n",
        "  from a GPU and dynamic memory allocation on a GPU. The standard pattern for\n",
        "  a lot of GPU work is:\n",
        "\n",
        "    1) Prepare the data on the host part of the program. By host we mean the\n",
        "       the CPU. In this case, the h_n integer is set to 19. The h_ prefix\n",
        "       indicates a host variable, i.e. one that we will use with the CPU side\n",
        "       of the program.\n",
        "    2) Allocate memory on the device. By device we mean GPU. In this case a\n",
        "       single integer, identified by d_n, is allocated using cudaMalloc. The\n",
        "       d_ prefix indicates a device variable, i.e. one that we will use with\n",
        "       the GPU side of the program.\n",
        "    3) Transfer data from the host to device. In this case cudaMemcpy is used\n",
        "       to copy the contents of h_n to d_n.\n",
        "    4) The kernel function is invoked. In this case the kernel function is\n",
        "       called kernel and is defined as __global__ which means a function\n",
        "       that will execute on the device but is invoked from the host. The\n",
        "       <<<1,1>>> part indicates that we want to execute the kernel with one\n",
        "       thread block consisting of one thread. The kernel function here will\n",
        "       only be invoked once in total.\n",
        "    5) The kernel function is executed, which in this case sets the contents\n",
        "       of the memory pointed to by d_n to 97.\n",
        "    6) Data is copied from the device to the host. In this case the contents\n",
        "       of memory pointed to by d_n are copied into the h_n variable.\n",
        "    7) Dynamically allocated memory is freed using cudaFree.\n",
        "    8) Results are output. In this case the value of h_n is printed, and if\n",
        "       all goes well should print 97.\n",
        "\n",
        "  CUDA functions return an integer code. If this code is not equal to zero\n",
        "  something has gone wrong. cudaGetErrorString returns a description of an\n",
        "  error given its code This program is rather paranoid and checks\n",
        "  the return codes of all call CUDA function calls and terminates the program\n",
        "  if zero was not returned.\n",
        "\n",
        "  To compile:\n",
        "    nvcc -o 01 01.cu\n",
        "\n",
        "  Dr Kevan Buckley, University of Wolverhampton, 2018\n",
        "******************************************************************************/\n",
        "\n",
        "__global__ void kernel(int *n){\n",
        "  *n = 97; // this is an arbitary number, just to see some results.\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  cudaError_t error;\n",
        "  int *d_n;\n",
        "  int h_n = 19;\n",
        "\n",
        "  error = cudaMalloc(&d_n, sizeof(int));\n",
        "  if(error){\n",
        "    fprintf(stderr, \"cudaMalloc on d_n returned %d %s\\n\", error,\n",
        "      cudaGetErrorString(error));\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  error = cudaMemcpy(d_n, &h_n, sizeof(int), cudaMemcpyHostToDevice);\n",
        "  if(error){\n",
        "    fprintf(stderr, \"cudaMemcpy to d_n returned %d %s\\n\", error,\n",
        "      cudaGetErrorString(error));\n",
        "  }\n",
        "\n",
        "  kernel <<<1,1>>>(d_n);\n",
        "  cudaThreadSynchronize();\n",
        "\n",
        "  error = cudaMemcpy(&h_n, d_n, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "  if(error){\n",
        "    fprintf(stderr, \"cudaMemcpy to h_n returned %d %s\\n\", error,\n",
        "      cudaGetErrorString(error));\n",
        "  }\n",
        "\n",
        "  error = cudaFree(d_n);\n",
        "  if(error){\n",
        "    fprintf(stderr, \"cudaFree on d_n returned %d %s\\n\", error,\n",
        "      cudaGetErrorString(error));\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  printf(\"result: h_n = %d\\n\", h_n);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwyuKwL1RVyf",
        "outputId": "7522fb01-0054-4e57-bf6b-ed77f5a23b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 01.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o 01  01.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBaSPNBZRzdc",
        "outputId": "a1f7db21-53bc-40cf-996e-ed6ce40d1d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[K01.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K01.cu:68:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KcudaError_t cudaThreadSynchronize()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   68 | \u001b[01;35m\u001b[K  cudaThreadSynchronize\u001b[m\u001b[K();\n",
            "      |   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:1069:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " 1069 | extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaThread\u001b[m\u001b[KSynchronize(void);\n",
            "      |                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuSl53WYR3mR",
        "outputId": "50e7b5f8-1a4f-4e6f-c2da-d53dba1b7c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: h_n = 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 01b.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "\n",
        "/******************************************************************************\n",
        "  Very simple CUDA program that shows the principles of copying data to and\n",
        "  from a GPU and dynamic memory allocation on a GPU. The standard pattern for\n",
        "  a lot of GPU work:\n",
        "\n",
        "    1) Prepare the data on the host part of the program. By host we mean the\n",
        "       the CPU. In this case, the h_n integer is set to 19. The h_ prefix\n",
        "       indicates a host variable, i.e. one that we will use with the CPU side\n",
        "       of the program.\n",
        "    2) Allocate memory on the device. By device we mean GPU. In this case a\n",
        "       single integer, identified by d_n, is allocated using cudaMalloc. The\n",
        "       d_ prefix indicates a device variable, i.e. one that we will use with\n",
        "       the GPU side of the program.\n",
        "    3) Transfer data from the host to device. In this case cudaMemcpy is used\n",
        "       to copy the contents of h_n to d_n.\n",
        "    4) The kernel function is invoked. In this case the kernel function is\n",
        "       called kernel and is defined as __global__ which means a function\n",
        "       that will execute on the device but is invoked from the host. The\n",
        "       <<<1,1>>> part indicates that we want to execute the kernel with one\n",
        "       thread block consisting of one thread. The kernel function here will\n",
        "       only be invoked once in total.\n",
        "    5) The kernel function is executed, which in this case sets the contents\n",
        "       of the memory pointed to by d_n to 97.\n",
        "    6) Data is copied from the device to the host. In this case the contents\n",
        "       of memory pointed to by d_n are copied into the h_n variable.\n",
        "    7) Dynamically allocated memory is freed using cudaFree.\n",
        "    8) Results are output. In this case the value of h_n is printed, and if\n",
        "       all goes well should print 97.\n",
        "\n",
        "  This version does no checking for errors. Its purpose is to show the main\n",
        "  functionality. For the \"real\" version of the program error checking should be\n",
        "  included. See 01.cu for this.\n",
        "\n",
        "  To compile:\n",
        "    nvcc 01b.cu\n",
        "\n",
        "  Dr Kevan Buckley, University of Wolverhampton, 2018\n",
        "******************************************************************************/\n",
        "\n",
        "__global__ void kernel(int *n){\n",
        "  *n = 97; // this is an arbitary number, just to see some results.\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  int *d_n;\n",
        "  int h_n = 19;\n",
        "\n",
        "  cudaMalloc(&d_n, sizeof(int));\n",
        "\n",
        "  cudaMemcpy(d_n, &h_n, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "  kernel <<<1,1>>>(d_n);\n",
        "\n",
        "  cudaThreadSynchronize();\n",
        "\n",
        "  cudaMemcpy(&h_n, d_n, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  cudaFree(d_n);\n",
        "\n",
        "  printf(\"result: h_n = %d\\n\", h_n);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzoHifJ6SHRH",
        "outputId": "ddefaa68-56cf-4eab-8943-7fa3d903b0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 01b.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o 01b  01b.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w86ZYjeNSU-f",
        "outputId": "f8c63399-eb80-4d10-f21a-3d379a6457e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[K01b.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K01b.cu:57:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KcudaError_t cudaThreadSynchronize()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   57 | \u001b[01;35m\u001b[K  cudaThreadSynchronize\u001b[m\u001b[K();\n",
            "      |   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:1069:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " 1069 | extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaThread\u001b[m\u001b[KSynchronize(void);\n",
            "      |                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./01b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EpqD1__SUw2",
        "outputId": "c5fc4fa1-134f-428e-805b-4a0e76a8c947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: h_n = 97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 02.cu\n"
      ],
      "metadata": {
        "id": "zmp72bwRSfhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}